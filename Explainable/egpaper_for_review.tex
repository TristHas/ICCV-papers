\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Assisting human experts in the interpretation of their visual process: A case study on assessing copper surface adhesive potency}%

\author{
Tristan Hascoet\\
Kobe University\\
{\tt\small tristan@people.kobe-u.ac.jp}
\and
Deng Xuejiao\\
Kobe University\\
{\tt\small dengxuejiao1005@yahoo.co.jp}
\and
Kiyoto Tai\\
MEC\\
%email\\
%{\tt\small xxx}
\and
Sachiko Nakamura\\
MEC\\
%email\\
%{\tt\small xxx}
\and
Tomoko Hayashi\\
MEC\\
%email\\
%{\tt\small xxx}
\and
Mari Sugiyama\\
MEC\\
%email\\
%{\tt\small xxx}
\and
Yasuo Ariki\\
Kobe University\\
%xxx\\
%{\tt\small xxx}
\and
Tetusya Takiguchi\\
Kobe University\\
%xxx\\
%{\tt\small xxx}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
Deep Neural Networks are often though to lack interpretability due to the distributed nature of their internal representations. In contrast, humans can generally justify, in natural language, for their answer to a visual question with simple common sense reasoning. However, human introspection abilities have their own limits as one often struggles to justify for the recognition process behind our lowest level feature recognition ability: for instance, it is difficult to precisely explain why a given texture seems more characteristic of the surface of a finger nail rather than plastic bottle.
In this paper, we showcase an application in which deep learning models can actually  help human experts justify for their own low-level visual recognition process: We study the problem of assessing the adhesive potency of copper sheets from microscopic pictures of their surface . Although highly trained material experts are able to qualitatively assess the surface adhesive potency, they are often unable to precisely justify for their decision process. We present a model that, under careful design considerations, is able to provide visual clues for human experts to understand and justify for their own recognition process. 
Not only can our model assist human experts in their interpretation of the surface characteristics, 
we show how this model can be used to test different hypothesis of the copper surface response to different manufacturing processes. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

% Human explanation ability & deep learning flaws.
Humans are experts in communicating the reasoning process behind their answer to visual questions.
For instance, on typical Visual Question Answering (VQA) samples, 
human annotators are often able to precisely justify in natural language the reason 
behind their answer to a certain visual question using simple common sense reasoning.
%Common sense includes spatial and causal relationships
In contrast, deep Learning models are often viewed as black box predictors lacking interpretability 
in the sense that existing tools often fail to explain the decision making process behind the model’s predictions.
For instance, a deep learning model trained end-to-end on a VQA dataset may be able to provide the same answer as its
human counterpart, but xxx.

% Limits of low-level introspection
While it is true that humans can justify for their answers on high level reasoning tasks, 
humans also often fail to explain the process behind their low-level feature recognition ability:
for example, precisely defining the nature of a specific texture (what are the defining features of a plastic or wooden surface?) or specific low-level part attributes exhibiting large intra-class variations (what is the defining features of a "leg" or a "wing"?).
Humans constantly perform such low-level visual recognition tasks while being unable to precisely justify for their own recognition process.

% Current problem
In this paper, we present one very practical instance of such situation in the micro-processor chip industry, 
in which expert material scientists are tasked with assessing the adhesive potency of copper sheets.
We propose a model that, under careful design considerations, is able to provide visual clues 
for human experts to understand and justify for their decision process.

% Ability of our proposed model.
The proposed model is designed so that a subset of its internal representations carry semantically meaningful 
Information that can be visualized and easily interpreted by humans.
Providing these visual clues, however, comes with the cost of imposing additional constraints on the architecture,
which we found to degrade the model accuracy:
Indeed, we found that networks with unrestricted architectures, 
(which do not provide interpretable features)
perform better than network architectures restricted so as to provide 
semantically meaningful representations.
This is because, as we restrict the architecture of the model, 
we formulate an assumption on the impact of the manufacturing process 
on the surface statistics which may not hold in reality.
This result suggest an inherent trade-off between the 
expressivity of an architecture and the explainability of its process.

%the human experts, confirming their intuitions 
%and eventually shading light on their own decision processes and biases
% Test hypothesis using generalization performance as a metric of hypothesis validity
While the degradation of the model accuracy is problematic from a performance perspective,
it offers an interesting opportunity from an explainability perspective:
As the model accuracy degrades due to the inadequacy of the assumption implicitly formulated by the model architecture,
we can use the model accuracy as a proxy metric for the adequacy of different assumptions.
This allows us to quantitavely assess different assumptions regarding the impact 
of manufacturing processes on the copper surface. 
This may prove useful to quantify the impact of manufacturing process on cooper surface adhesive potency
and eventually help optimize the manufacturing process.

% Learn a hypothesis/theoretical model
%Going one step further we show how, given sufficient experimental data, the model
%can be augmented to automatically learn and formulate these hypothesis on itself.
% Conceptual contribution
In essence, the argument this paper is aiming for is as follows: although deep learning models 
lack the "common sense reasoning” abilities and the powerful formalism of natural language to communicate 
and justify for their decision making process, they can provide powerful to explain low-level recognition processes.

% Practical contribution
In practice, the contribution of this paper is as follows:
\begin{enumerate}
\item  We formalize a segmentation procedure based on a probabilistic weak label segmentation framework.
\item  We introduce a formalism to show how the model accuracy can be used as a proxy metric to quantify the validity of different assumptions
on the dataset.
\end{enumerate}

% Paper organization
The remainder of this paper is organized as follows:
In Section 2, we present some background information on the motivation for this project:
We start by discussing the importance of copper surface adhesive potency,
and detail the dataset used in our experiments. 
Section 3 details our contribution.
Section 4 briefly relates our work to different research topics and Section 5 presents the results of our experiments.
Finally, Section 6 further discusses the relevance of our results, insisting on the limitations of our assumptions to conclude this paper.

%------------------------------------------------------------------------
\section{Background}

%% Catchy intro about the importance of chips
Electric circuits are core to a wide variety of electronic devices including industrial and household appliances (e.g. fridges and microwaves), mobile communication devices and automobiles.
%% Glue copper and resin together
To propagate electric signals between various components, electronic circuits rely on an electronic substrate  made of copper wires through which electricity flows, isolated from each other by insulators (resin, resist, prepreg, etc.).
Electronic substrates are organized in a multi-layered structure in which each layer contains different wire connections that need to be properly isolated from each other. Figure 1 illustrates the organization of such an electric circuit.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
\caption{
Illustration of a typical eletric circuit board.
}
\end{figure}


% Problem of peelin off
The bulit-in copper microstructures can be easily peeled off from their isolating resin due to weak adhesion. For example, this can happen because of the impact of dropping a smartphone, or due to an excess of heat generated by running heavy computations.
Such disadhesion of copper wires with their isolating resin may result in electrical short circuits in the electronic substrate and break the device.
Hence a strong metal-to-resin bonding is necessary in order to enhance the reliability of electronic devices.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure2"}
	\caption{
		Illustration of xxx
	}
\end{figure}

% Copper surface importance
Among other factors, the achievable strenght of the bond between the copper and the isolating resin is determined by the characteristics of the copper surface, as illustrated in Figure 2. In very broad terms, rough surfaces allow for stronger bonds as the asperities of the surface provide mechanical support against friction forces. In contrast, smooth surfaces provide little support against friction so that they have lower adhesive potential. 
In the remaining of this paper, we will refer to the potential bonding strength of a copper surface as its "adhesive potency".
It is also important to note that the adhesive potency of a copper surface is related to its "roughness", which is observable at the microscopic scale.

% Difficulty of assessment
Electronic substrate manufacturers have developped advanced manufacturing processes to shape the surface of copper sheets in order to increase their adhesive potency. This is typically achieved by applying a very small amount of a corrosive solution on the copper surface.
Being able to accurately assess the adhesive potency of a copper surface would allow to further optimize manufacturing processes to increase the reliability of electronic devices.
However, assessing the adhesive potency of a copper surface is a complex task, even for the most expert practinioners. 
Hence the motivations of this study is two fold: 
First we aim to automate the evaluation of a copper sheet adhesive potency from microscopic imaging of its surface. 
Second, we aim to understand the underlying principles by which the xxx 
Towards this goal, we built a dataset of microscopic images of copper surfaces, which we detail below.

%% Dataset
We imaged copper surfaces using Scanning Electron Microscopy (SEM) at a resolution of $xxx$ micro meters.
To investigate the impact of different manufacturing processes on the copper surface, 
we applied 16 different corosive solutions, with decreasing corrosive power, to the copper surface.
For each of these solutions, we captured 50 SEM images of $xxx \times xxx$ pixels so that the full dataset
consists of $800$ ($16 \times 50$) images.
%The corrosive power of each solution was degraded by a constant factor $\delta$ between each 
Each image is annotated with a label $y_i \in Y$ corresponding to the corrosive solution used to shape the copper surface.
Each solution $y_i$ was obtained by submitting the original solution $y_0$ to an extreme stress test for a period of time $i \times T$.
Hence, we know that for all $i$ images of copper surfaces with label $y_i$ show higher adhesive potency than the images labelled with $y_{i+1}$.
However, we do not know the \textit{exact} impact of the stress test on the surface adhesive potency.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure2"}
	\caption{
		Illustration of xxx
	}
\end{figure}

\section{Method}

\subsection{Dataset and Notations}

In all our following experiments, we have split the full dataset $\mathcal{D}$ into a training, validation and test set $\mathcal{D}=\mathcal{D}_{tr} \cup \mathcal{D}_{val} \cup \mathcal{D}_{te}$ so that the number of images per label $y_i$ in each set is 40, 5 and 5 respectively.

\subsection{Baseline}

We start by establishing a strong baseline for our study.
The baseline architecture follows standard convolutional network designs for image classification.
This architecture, illustrated in Figure 4, is made of several residual blocks 
sequentially interleaved with max pooling operations.
Each residual block consists of $n$ repetitions of a sequence of 
$3 \times 3$ Convolution, Batch Normalization and ReLU layers,
followed by a residual skip connection.
We set $N$ residual blocks between every max pooling layer
and we denote by $d$ the number of pooling layers.
Hence, the full depth $D$ of the network (in number of convolution layers) 
is given by $D=d \times n \times N +1$, where the term $1$ corresponds to the
initial $3 \times 3$ Convolution layer happening before the first pooling operation.
Finally, the top layer of the network is made of a global average pooling layer 
followed by a linear softmax layer with output dimension 15 corresponding to our number of classes.
For simplicity and contrary to standard practices, we keep the number of channels $c$ constant 
in all layers of the network.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Illustration of our baseline architecture.
	}
\end{figure}

With this parameterization, our network is fully specified by the four hyper parameters $c$, $d$, $n$ and $N$.
We performed a grid search over these hyper parameters to select the best performing architecture.
The details of this architecture search are given in the experiment section,
and, as we shall see then, the best performing architecture performs significantly better than human experts.
However, the decision process through which this model reaches such a high accuracy is entirely opaque
as the distributed nature of the model's internal representations provides little interpretability.
The remainder of this section details our attempt to design an architecture that can provide 
useful explanations of the process through which high accuracy recognition can be performed.

\subsection{Assumptions}
% Basic idea
%We start by formally stating the assumptions on which rely our analysis.
%Our proposed method is based on the idea that classifying 
The basic idea behind our method is similar to the visual insights provided by the
visualization method of \cite{} and the attention mechanisms in visual models 
for image captioning and question answering tasks:
We would like to evaluate the contribution of each input pixel to the final classification decision. 
%Having such a visual explanation could help us explain the model's decision process xxx

To do so, we modify our initial problem formulation into a segmentation task:
Given an input $x \in \mathbb{R}^{h \times w}$, we want to design a model that outputs 
a segmentation mask $s \in \mathbb{R}^{h \times w}$ assessing the contribution of each inidivual pixel to the output adhesive potency score (i.e., the output class $y$).
Training a typical segmentation model for this task would require ground truth 
segmentation masks $s$ for each image $x$ of the training set.
However, manually annotating ground truth segmentation masks for this task is not feasible, 
as human experts are not able to provide such fine-grained annotations.
Instead, we have to train the segmentation given a single groud truth label $y_i$ per image.

In order to do so, we make the following assumption:
\textbf{Assumption 1}: 
For a given image $x$ with label $y$, 
the pixel-wise values of the true (unknown) segmentation mask $s$
follow a spatially stationnary binomial distribution whose expected value, 
averaged over the spatial dimensions of $x$, is given by $y$ following:
$$
s_{hw} \tilde{a} \mathbb{B}(f(y)),
$$

in which we introduced a target function $f$, which we will discuss below.

%For a given image, we suppose the existence of an unknown ground truth binary that assigns a value of 1 to regions of
In other words, we suppose that there exists a true binary segmentation map
s assigning to each individual pixel of $x$ an adhesive potency score of 0 
in regions of the surface providing low adhesive potency (i.e. smooth copper surface areas)
and a value of 1 in regions of the surface providing high adhesive potency (i.e. rough copper surface areas).
The adhesive potency score of an entire image is then given by the average of its pixel-wise score,
and this average value is given by the image label $y$.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Illustration of the segmentation model architecture.
	}
\end{figure}

Figure xxx provides a visual illustration of this idea.
For each image, we suppose the existence of such binary mask $s$
quantifying the adhesive potency of local areas of the surface.
The average value of the binary mask $s$ amounts to the ratio of the
copper surface covered in white (i.e. with value 1, corresponding to high adhesive potency).
Our assumption means that this ratio stays constant for different images $x$ sharing a similar label $y$,
and takes a value given by the target function $f$.

\subsection{Architecture}
% 
%We modify the baseline architecture presented in Section xxx to output binary segmentation maps.
In this section, we present the architecture we use 
to compute binary segmentation masks from input images.
Our architecture extends the baseline architecture presented in Section xxx,
with an ascending path that progressively upsamples the output
of the descending path, similar to the Unet architecture \cite{}.
The expanding path is the exact symmetric of the descending path,
with bilinear upsampling layers instead of max pooling.
Different from the UNet architecture, and following previous works \cite{},
we merge the outputs of the descending path modules with the inputs of the ascending path 
by summation instead of concatenation and use valid convolutions 
to preserve the spatial resolution of the output.
Finally, we add a sigmoid layer after the last convolutions to bound the output values between 0 and 1.
Figure xxx illustrates this architecture.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Illustration of the segmentation model architecture.
	}
\end{figure}

\subsection{Loss function}

Given a model $M_{\theta}$, and a training dataset of labeled samples $\mathcal{D_{tr}}={(x_i, y_i)}$,
learning is done by minimizing the following expectation over the model's parameters $\theta$:

$$\theta^* = argmin_{\theta} \mathbb{E}_{x,y \in \mathcal{D_tr}} ||m_{\theta}(x) - f(y) ||^2 $$

AS we shall see in the next subsection, we use a hypothesis function $g$ 
as an approximation of the true target function $f$, to which we do not have access, 
so that the actual training loss used to learn the model parameters $\theta$ is:

$$\theta^* = argmin_{\theta} \mathbb{E}_{x,y \in \mathcal{D_tr}} ||m_{\theta}(x) - g(y) ||^2 $$

\subsection{Hypothesis function}

% Intro
In section xxx, we have made the assumption 
that labels $y$ provide us with the expected value 
of the true binary segmentation masks through a target function $f$.
In this section, we discuss the role of this target function.

% What does this target mean?
The target function $f$ defines the evolution of the copper surface adhesive potency with $y$ as the surface ratio of high adhesive potency $\mathbb{E}_{h,w}s_{hw}$.
$f$ is an ideal function of which we have supposed the existence, 
but to which we can not have access,
as we do not know the influence of the manufacturing process on the copper's surface.

We thus introduce a hypothesis function $g$ approximating $f$,
which we aim to estimate from the labeled data.
Although we do not have access to $f$, we know several of its characteristics,
which we can use to reduce the search space of hypothesis functions $g$.

In section xxx, we established that for all $i$, copper surfaces with label $i+1$ have lower 
adhesive $i-1$ have lower adhesive potency than copper sheets of label $i$.
Hence, we know that $f$ is a monotonically decreasing function.
We can thus restrict our search of hypothesis function $g$ to monotonically decreasing function.
Second, as $f$ describes the ideal, true evolution of the copper surface statistics with $y$,
so that training an ideal model with the supervision signal provided by $f$ should yield to zero test errors.
Hence training a segmentation model with a hypothesis $g \approx f$ should lead to low test errors.
This means that we can use the error of the model on the held out validation dataset as a proxy measure on
the validity of the hypothesis function $g$.

In the experiment section, we evaluate the model errors with different hand-crafter hypothesis functions $g$.
In future work, we aim to learn the hypothesis function $g$ together with the model parameters.


\section{Related Work}

% Intro
We identify three different lines of research that share similarities with our study:
Explainabality of learned visual representations, 
weak supervision of segmentation models and 
machine learning applications for material science.
Our can be seen as a weakly supervised approach to provide material 
experts with strong supports for interpretable decision, 
which falls at the intersection of these three research lines.
We briefly present some of these works in the following subsections

\subsection{Explaining CNN representations}
Although deep neural networks have achieved a great success on a variety 
of challenging visualization tasks in recent years, 
our understanding of how these neural network models is far from enough to interpret.
The pursuit of figuring out what is learned for each layer of models 
and how trained neural network models really "think" never stops.

%% Efforts to understand hidden representations
% Visualization works.
 % Jason Yosinski et al.,2015,understanding neural network through deep visualization
Jason Yosinski et al. provide two useful tools for visualizing and interpreting neural nets. 
One is to visualize  the activations produced on each layer of a trained convnet as it 
processes an image or video, and the other enables visualizing 
features at each layer of a DNN via regularized optimization in image space.
 % activation atlas https://distill.pub/2019/activation-atlas/
 %%(Shan Carter et al.2019)
Shan Carter et al. create an explorable activation atlas of features the network has learned, by using feature inversion to visualize millions of activations from an image classification network, which can reveal how the network typically represents some concepts.
 % attention models for image captioning and visual question answering
   %Yashi Goyal et al., CVPR 2017, Marking the V in VQA Matter:Elevating the Role of Image Understanding in Visual Question Answering
   %Peng Zhang et al., CVPR 2016, Yin and Yang:Balancing and Answering Binary Visual Questions
   %Aishwarya Agrawal et al., ICCV 2015, VQA:Visual Question Answering
There is a new dataset called Visual Question Answering(VQA) containing open-ended questions about images.These questions require an understanding of vision, language and commonsense knowledge to answer.
 % Trevor darrel train models to explain their decision 
  %Attentive Explanations: Justifying Decisions and Pointing to the Evidence, Dong Huk Park, 2017
Trevor Darrel et al. build  models (such as Pointing and Justification-based explanation model) to explain their decisions, generating convincing explanations.
TensorFlow provides an attention-based model, which enables us to see what parts of the image the model focuses on as it generates a caption.
%% Deep learning for scientific discovery

\subsection{Weakly Supervised Segmentation}

%% Segmentation from lazy labels.
An other point we should pay attention to is different labelling strategies in conventional supervised learning.
Typically speaking, it's often assumed that each instance is associated with one single label. However, there should usually be more than one labels for one instance in real-world tasks, if it is multi-label learning.

% Detection with weak labels
%%Matthew B.Blaschko et al., 2010,Simultaneous Object Detection and Ranking with Weak Supervision

%%Ross B. Girshick et al., 2011,Object Detection with Grammar Models
%%Bharath Hariharan et al.,2014,Simultaneous Detection and Segmentation
%%Hakan Bilen et al.,2016,Weakly Supervised Deep Detection Networks

% segmentation weak label https://arxiv.org/pdf/1904.01636.pdf
%%Eugene Vorontsov et al., 2019, Boosting segmentation with weak supervision from image-to-image translation
Eugene et al. propose a semi-supervised framework that employs image-to-image translation between weak labels.
% Semi-supervised learning for object detection or semantic segmentation
%%Yuxing Tang et al.,2016, Large Scale Semi-supervised Object Detection using Visual and Semantic Knowledge Transfer
Yuxing Tang et al. build a similarity-based knowledge transfer mode trying to investigate whether knowledge about visual and semantic similarities of object categories can help improve the peformance of detectors trained in a weakly supervised setting.

% Our work = Merge segmentation from lazy labels and scientific discovery
%% Rihuan Ke et al., 2019, A multi-task U-net for segmentation with lazy labels
The most related work with ours is Rihuan Ke's[*] in which they present a semi-supervised learning strategy for segmentation with lazy labels and develop a multi-task learning framework to integrate the instance detection, separation and segmentation within a deep neural network.

\subsection{Machine Learning for Material Science}

Deep learning has showed remarkable success on many important 
learning problems in chemistry, drug discovery, biology and materials science.

In the field of materials science, deep neural networks have also been receiving inreasing attention and have achieved great improvements, for example, in material property prediction and new materials discovery for batteries.
% Material sciences
% Discover new materials or properties (messaging passing deep networks by gilmer)
%%Justin Gilmer,2017, Neural Message Passing for Quantum Chemistry
%A general framework for supervised learning on graphs called Message Passing Neural Networks (MPNNs) is widely used on neural computation for predicting quantum states of molecules.
   % Drug discovery: Find molecules useful by predicting their property
   %%Dibyendu Dana et al.,2018, Deep Learning in Drug Discovery and Medicine;Scratching the Surface
   %%Jessica Vamathevan et al.,2019, Applications of machine learning in drug discovery and development
%Drug discovery is also benifiting from the development of artificial intelligence technology, which is automating the invention of new chemical entities and the mining of large databases, in drug design and molecular medicine field.  
   % Protein folding
%Deep learning approaches such as DeepMind's AlphaFold are helpful for scientists to deal with the "protein folding problem", not only predicting the intricate 3D structure of a protein but also predicting the physical properties of a protein structure, and are making significant progress on one of the core challenges in biology.
% THIS ONE New Materials for batteries
%% 

\section{Experiments}

\subsection{Classification results}
% xxx
We start by evaluating the baseline classification model described in Section xxx and illustrated in Figure xxx.
The hyperparameters $n$, $N$, $d$, and $c$ of the classifier architecture 
were obtained by a grid search within the limits of a 12GB Nvidia GPU memory size .
The model was trained on the training dataset for 200 epochs using the Adam optimizer with default parameterization.

We compare the classification outputs of the baseline model to those 
of an expert material scientist on a blind test.
The human expert was given the sample images of the training set to practice,
and we evaluated the expert's answers on the samples of the test set.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Results of the human evaluation.
	}
\end{figure}

Figure xxx shows the results achieved by the model and Figure xxx shows the results achieved by our baseine model.
As can be seen in these figures, the model tends predict the manufacturing 
process more accurately than the human evaluator.
However, these results should be taken with a grain of caution 
as the human expert was given little to practice on this specific dataset,
while the model was selected as the best performing baseline from an extensive parameter search.
We plan on reconducting the human evaluation in the updated version of this paper.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Results of the model evaluation.
	}
\end{figure}

Interestingly, however, the human expert seems to accurately recognize 
the surfaces of highest adhesive potency as seen in the upward trends of his results 
for low $y$ values. For example, he easily identified the surface for $y=0$.
On the other hand, his guesses for low adhesive potency are much more random.

This is in stark contrast with the classifier accuracy, which perfectly identifies
the low adhesive potency surfaces (high $y$ values), but consistently misclassifies 
surfaces of high adhesive potency (i.e.; for $y=1$ and $y=2$).

\subsection{Visualization of Segmentation Results}

% Visualize
To motivate our results, Figure xxx illustrates an output of the segmentation model on a small patch of an input image $x$.
To a novice observer, the model seems to assign lower adhesive potential to smoother regions of the input.
In futur work, we plan on further exploring these visualizations to better understand the 
patterns characterizing the surface adhesive potency.


\subsection{Investigation of hypothesis functions}

% Train with simplest hypothesis


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"./figures/Figure1"}
	\caption{
		Results of the model evaluation.
	}
\end{figure}

% Fine-tune on other hypothesis.



\section{Discussion \& Conclusion}

% Summary of the paper



% Success of the classification



% Potential help of visualization



% Loss of accuracy of the model



% Constraining 



% Limits of the assumption





{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
